{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Snakemake Pipeline\n",
    "\n",
    "This tutorial covers how to use Neurodent's Snakemake pipeline for large-scale, reproducible EEG analysis workflows.\n",
    "\n",
    "## What is Snakemake?\n",
    "\n",
    "[Snakemake](https://snakemake.readthedocs.io/) is a workflow management system that creates reproducible, scalable data analyses. Key features:\n",
    "\n",
    "- **Automatic dependency tracking**: Only recomputes what's needed when data or parameters change\n",
    "- **Parallel execution**: Processes multiple samples simultaneously\n",
    "- **Fault tolerance**: Resumes from where it left off after failures\n",
    "- **Resource management**: Specify memory, CPU, and time requirements for each step\n",
    "- **Cluster integration**: Seamlessly scales from laptops to HPC clusters\n",
    "\n",
    "## Why Use the Snakemake Pipeline?\n",
    "\n",
    "For experiments with many animals and days of recordings, manual analysis becomes impractical. The Neurodent pipeline automates:\n",
    "\n",
    "1. Raw EEG data → Windowed Analysis Results (WARs)\n",
    "2. Quality filtering and artifact removal\n",
    "3. Diagnostic figure generation\n",
    "4. Statistical analyses and publication-ready plots\n",
    "5. All intermediate processing steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Ensure you have Neurodent and Snakemake installed:\n",
    "\n",
    "```bash\n",
    "# Install both with uv\n",
    "uv add neurodent snakemake\n",
    "\n",
    "# Or with pip\n",
    "pip install neurodent snakemake\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Overview\n",
    "\n",
    "The pipeline processes data through multiple stages:\n",
    "\n",
    "```\n",
    "Raw EEG Data\n",
    "    ↓\n",
    "WAR Generation (windowed features)\n",
    "    ↓\n",
    "Quality Filtering (remove bad days/genotypes)\n",
    "    ↓\n",
    "Standardization (channel ordering)\n",
    "    ↓\n",
    "Fragment Filtering (temporal artifacts)\n",
    "    ↓\n",
    "Channel Filtering (spatial artifacts)\n",
    "    ↓\n",
    "WAR Flattening (aggregate across time)\n",
    "    ↓\n",
    "Zeitgeber Analysis (circadian features)\n",
    "    ↓\n",
    "Statistical Figures & Plots\n",
    "```\n",
    "\n",
    "Each stage produces intermediate files that are automatically cached. If you modify parameters or data, Snakemake only reruns affected steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The pipeline requires two configuration files in the `config/` directory:\n",
    "\n",
    "### 1. `config/config.yaml` - Analysis Parameters\n",
    "\n",
    "This file defines paths, analysis settings, and computational resources:\n",
    "\n",
    "```yaml\n",
    "# Core directories\n",
    "base_folder: \"/path/to/neurodent/output\"\n",
    "data_parent_folder: \"/path/to/raw/eeg/data\"\n",
    "temp_directory: \"/path/to/temp/storage\"\n",
    "\n",
    "# Sample configuration\n",
    "samples:\n",
    "  samples_file: \"config/samples.json\"\n",
    "  quality_filter:\n",
    "    exclude_unknown_genotypes: true\n",
    "    exclude_bad_animaldays: true\n",
    "\n",
    "# Analysis parameters\n",
    "analysis:\n",
    "  war_generation:\n",
    "    mode: \"nest\"\n",
    "    lro_kwargs:\n",
    "      mode: \"bin\"\n",
    "      multiprocess_mode: \"dask\"\n",
    "  \n",
    "  fragment_filter_config:\n",
    "    logrms_range:\n",
    "      z_range: 3\n",
    "    high_beta:\n",
    "      max_beta_prop: 0.4\n",
    "  \n",
    "  channel_filter_config:\n",
    "    lof:\n",
    "      reject_lof_threshold: 2.5\n",
    "      min_valid_channels: 3\n",
    "\n",
    "# Resource requirements (adjust for your system)\n",
    "cluster:\n",
    "  war_generation:\n",
    "    time: \"3h\"\n",
    "    mem_mb: 70_000\n",
    "    threads: 10\n",
    "```\n",
    "\n",
    "Key parameters:\n",
    "- **Paths**: Set `base_folder`, `data_parent_folder`, and `temp_directory` for your system\n",
    "- **Analysis**: Configure filtering thresholds, detection parameters, etc.\n",
    "- **Resources**: Memory, time, and CPU requirements (used for cluster scheduling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. `config/samples.json` - Sample Metadata\n",
    "\n",
    "This file defines which animals to process and their experimental metadata:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"data_parent_folder\": \"/path/to/raw/eeg/data\",\n",
    "  \"data_folders_to_animal_ids\": {\n",
    "    \"experiment1_cohort1\": [\"M1\", \"M2\", \"F1\", \"F2\"],\n",
    "    \"experiment2_cohort2\": [\"M3\", \"M4\", \"F3\"]\n",
    "  },\n",
    "  \"GENOTYPE_ALIASES\": {\n",
    "    \"MWT\": [\"M1\", \"M3\"],\n",
    "    \"FWT\": [\"F1\", \"F3\"],\n",
    "    \"MMut\": [\"M2\", \"M4\"],\n",
    "    \"FMut\": [\"F2\"]\n",
    "  },\n",
    "  \"bad_channels\": {\n",
    "    \"experiment1_cohort1 M1\": {\n",
    "      \"M1 MWT Jan-08-2022\": [\"LHip\", \"RHip\"]\n",
    "    }\n",
    "  },\n",
    "  \"bad_folder_animalday\": [\n",
    "    \"experiment1_cohort1 M2\"\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Key sections:\n",
    "- **data_folders_to_animal_ids**: Maps recording folders to animal IDs\n",
    "- **GENOTYPE_ALIASES**: Groups animals by genotype for statistical comparisons\n",
    "- **bad_channels**: Per-session bad channel lists for manual artifact rejection\n",
    "- **bad_folder_animalday**: Complete animals/days to exclude from analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Pipeline\n",
    "\n",
    "### Basic Execution\n",
    "\n",
    "Run the pipeline on your local machine or compute node:\n",
    "\n",
    "```bash\n",
    "# Dry run to preview what will execute\n",
    "snakemake --dry-run\n",
    "\n",
    "# Execute with 4 parallel jobs\n",
    "snakemake --cores 4\n",
    "\n",
    "# Run a specific analysis stage\n",
    "snakemake --cores 4 results/wars_quality_filtered/\n",
    "\n",
    "# Force rerun of a specific rule and downstream steps\n",
    "snakemake --cores 4 --forcerun war_generation\n",
    "```\n",
    "\n",
    "### Useful Commands\n",
    "\n",
    "```bash\n",
    "# Visualize the workflow\n",
    "snakemake --dag | dot -Tpng > dag.png\n",
    "snakemake --rulegraph | dot -Tpng > rulegraph.png\n",
    "\n",
    "# Unlock directory after crash\n",
    "snakemake --unlock\n",
    "\n",
    "# Clean up specific outputs to rerun\n",
    "snakemake --delete-all-output\n",
    "\n",
    "# Show which files are missing\n",
    "snakemake --summary\n",
    "```\n",
    "\n",
    "### Monitoring Progress\n",
    "\n",
    "Check the `logs/` directory for detailed output from each rule:\n",
    "\n",
    "```bash\n",
    "# View real-time logs\n",
    "tail -f logs/war_generation_*.log\n",
    "\n",
    "# Check results directory\n",
    "ls -lh results/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on HPC Clusters\n",
    "\n",
    "For large-scale analyses, you can submit pipeline jobs to an HPC cluster scheduler. Snakemake supports multiple schedulers including SLURM, SGE, PBS, LSF, and others.\n",
    "\n",
    "### General Cluster Execution\n",
    "\n",
    "Snakemake can interface with any job scheduler using profiles. The key is to:\n",
    "\n",
    "1. Define resource requirements in `config.yaml` (time, memory, threads)\n",
    "2. Create a cluster profile for your scheduler\n",
    "3. Submit the pipeline using the profile\n",
    "\n",
    "Snakemake will automatically:\n",
    "- Submit each rule as a separate job\n",
    "- Manage job dependencies\n",
    "- Monitor job status\n",
    "- Resubmit failed jobs (with `restart-times`)\n",
    "\n",
    "### Example: SLURM Scheduler\n",
    "\n",
    "Here's how to set up Snakemake with SLURM (adapt for your scheduler):\n",
    "\n",
    "```bash\n",
    "# Create a SLURM profile (one-time setup)\n",
    "mkdir -p ~/.config/snakemake/slurm\n",
    "\n",
    "cat > ~/.config/snakemake/slurm/config.yaml << 'EOF'\n",
    "cluster: \"sbatch --time={cluster.time} --mem={cluster.mem_mb} --cpus-per-task={threads} --job-name={rule} --output=logs/{rule}_%j.out\"\n",
    "jobs: 100\n",
    "restart-times: 3\n",
    "EOF\n",
    "\n",
    "# Submit pipeline\n",
    "snakemake --profile slurm\n",
    "```\n",
    "\n",
    "The pipeline reads resource requirements from your `config.yaml`:\n",
    "\n",
    "```yaml\n",
    "cluster:\n",
    "  war_generation:\n",
    "    time: \"3h\"\n",
    "    mem_mb: 70_000\n",
    "    threads: 10\n",
    "```\n",
    "\n",
    "And Snakemake translates these to scheduler-specific flags.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring Cluster Jobs\n",
    "\n",
    "For SLURM:\n",
    "```bash\n",
    "# Check job queue\n",
    "squeue -u $USER\n",
    "\n",
    "# View job details\n",
    "sacct --format=JobID,JobName,State,Elapsed,MaxRSS\n",
    "\n",
    "# Monitor specific rule logs\n",
    "tail -f logs/war_generation_*.out\n",
    "```\n",
    "\n",
    "For SGE:\n",
    "```bash\n",
    "# Check job queue\n",
    "qstat -u $USER\n",
    "\n",
    "# View job details\n",
    "qacct -j <job_id>\n",
    "```\n",
    "\n",
    "For PBS:\n",
    "```bash\n",
    "# Check job queue\n",
    "qstat -u $USER\n",
    "\n",
    "# View job details\n",
    "qstat -f <job_id>\n",
    "```\n",
    "\n",
    "### Other Schedulers\n",
    "\n",
    "Snakemake supports many schedulers. See the [Snakemake cluster documentation](https://snakemake.readthedocs.io/en/stable/executing/cluster.html) for:\n",
    "- **SGE** (Sun Grid Engine)\n",
    "- **PBS/Torque**\n",
    "- **LSF** (Load Sharing Facility)\n",
    "- **Cloud executors** (Google Cloud, AWS, Azure)\n",
    "\n",
    "The general pattern is the same: create a profile with your scheduler's submission command and resource mappings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Outputs\n",
    "\n",
    "The pipeline creates a structured results directory:\n",
    "\n",
    "```\n",
    "results/\n",
    "├── wars_quality_filtered/       # Initial WARs with quality filtering applied\n",
    "├── wars_standardized/            # Channel-reordered WARs\n",
    "├── wars_fragment_filtered/       # After temporal artifact removal\n",
    "├── wars_flattened_manual/        # Aggregated with manual channel filtering\n",
    "├── wars_flattened_lof/           # Aggregated with LOF channel filtering\n",
    "├── diagnostic_figures/           # Per-animal QC plots\n",
    "│   └── {animal}/\n",
    "│       ├── unfiltered/           # Pre-filtering diagnostics\n",
    "│       └── filtered/             # Post-filtering diagnostics\n",
    "├── wars_zeitgeber/               # Circadian time-aligned features\n",
    "├── zeitgeber_plots/              # Temporal heatmaps\n",
    "├── relfreq_plots/                # Feature distributions\n",
    "├── ep_figures/                   # Statistical group comparisons\n",
    "├── ep_heatmaps/                  # Connectivity matrices\n",
    "├── lof_evaluation/               # LOF threshold optimization\n",
    "└── filtering_comparison_plots/   # Manual vs LOF comparison\n",
    "\n",
    "logs/\n",
    "└── {rule}_*.log                  # Execution logs for each rule\n",
    "```\n",
    "\n",
    "Key outputs:\n",
    "- **`.pkl` files**: Python pickled WARs with computed features\n",
    "- **`.json` files**: Metadata and analysis parameters\n",
    "- **Figures**: PNG/PDF diagnostic and publication plots\n",
    "- **CSV files**: Exportable tabular data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting & Best Practices\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**Pipeline is locked:**\n",
    "```bash\n",
    "snakemake --unlock\n",
    "```\n",
    "\n",
    "**Need to rerun everything:**\n",
    "```bash\n",
    "snakemake --forceall --cores 4\n",
    "```\n",
    "\n",
    "**Jobs failing due to insufficient memory:**\n",
    "Increase memory in `config.yaml` under `cluster` section for the failing rule.\n",
    "\n",
    "**Want to test changes without full pipeline:**\n",
    "```bash\n",
    "# Run just one rule\n",
    "snakemake --cores 4 war_generation\n",
    "\n",
    "# Or target specific output\n",
    "snakemake --cores 4 results/wars_quality_filtered/animal1/\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always dry-run first** to preview execution:\n",
    "   ```bash\n",
    "   snakemake --dry-run\n",
    "   ```\n",
    "\n",
    "2. **Test on small subset** before full dataset:\n",
    "   - Temporarily modify `samples.json` to include just 1-2 animals\n",
    "   - Run locally to verify parameters\n",
    "   - Expand to full dataset once validated\n",
    "\n",
    "3. **Monitor resource usage**:\n",
    "   - Check logs for memory errors\n",
    "   - Adjust `cluster` resources in `config.yaml` as needed\n",
    "   - Most rules should complete within their time limits\n",
    "\n",
    "4. **Keep intermediate files**:\n",
    "   - Don't delete results directories - Snakemake uses them for caching\n",
    "   - If rerunning from scratch, use `--forceall` instead of deleting\n",
    "\n",
    "5. **Version control your configs**:\n",
    "   - Track `config/config.yaml` and `config/samples.json` in git\n",
    "   - Document parameter changes for reproducibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you understand the Snakemake pipeline:\n",
    "\n",
    "- **Explore outputs**: Review diagnostic figures in `results/diagnostic_figures/` to assess data quality\n",
    "- **Customize analysis**: Modify parameters in `config/config.yaml` for your experimental needs\n",
    "- **Interactive analysis**: Load `.pkl` WAR files in Python for custom analyses\n",
    "- **Learn more**: Check out other tutorials:\n",
    "  - [Windowed Analysis](windowed_analysis.ipynb) - Understanding feature extraction\n",
    "  - [Visualization](visualization.ipynb) - Creating custom plots\n",
    "  - [Basic Usage](../quickstart/basic_usage.ipynb) - Interactive Python workflow\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Snakemake Documentation](https://snakemake.readthedocs.io/)\n",
    "- [Snakemake Cluster Execution](https://snakemake.readthedocs.io/en/stable/executing/cluster.html)\n",
    "- [Snakemake Profiles](https://github.com/Snakemake-Profiles) - Community-maintained scheduler profiles\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
